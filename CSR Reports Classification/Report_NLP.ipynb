{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sustainability reports & NLP  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "#### Corporate Social Responsibility Reports (CSR)\n",
    "A corporate social responsibility (CSR) report is an internal and external facing document companies use to communicate CSR efforts around environmental, ethical, philanthropic, and economic impacts on the environment and community.    \n",
    "\n",
    "#### Natural Language Processing (NLP)\n",
    "Natural language processing (NLP) is a field of linguistics and machine learning that deals with natural (i.e., human) languages. The goal is to \"understand\" the unstructured text data and produce something new. Examples of NLP tasks are language translation, text summarization, and sentiment analysis.  \n",
    "\n",
    "\n",
    "#### Zero-Shot Learning (ZSL)\n",
    "Human languages are really complex, so it is impossible to train classifiers on every single phrase. Zero-shot learning (ZSL) models allow classification of text into categories unseen by the model during training. These methods work by combining the observed/seen and the non-observed/unseen categories through auxiliary information, which encodes properties of objects.    \n",
    "\n",
    "Other common uses for zero-shot learning models are images and videos. And the uses keep growing, such as activity recognition from sensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import tika\n",
    "tika.initVM()\n",
    "from tika import parser\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import pipeline  # Hugging Face\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing CSR PDFs\n",
    "A non-trivial portion of classifying CSR reports is converting them to a computer-readable format. Companies publish their CSR reports as PDFs, which are notoriously hard to read. Our goal is to extract text as a list of sentences.  \n",
    "\n",
    "We will be doing very simple parsing of a PDF report using the package tika to extract the text, regular expressions to filter and join the text, and NLTK to split the text into sentences.  \n",
    "\n",
    "This is by no means the best way to do it, but it's relatively simple and gets the job done well enough for our purposes. Text cleaning is task-specific, so you need to consider what is sufficient for your problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parsePDF:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "    \n",
    "    def extract_contents(self):\n",
    "        \"\"\" Extract a pdf's contents using tika. \"\"\"\n",
    "        pdf = parser.from_file(self.url)\n",
    "        self.text = pdf[\"content\"]\n",
    "        return self.text\n",
    "        \n",
    "    \n",
    "    def clean_text(self):\n",
    "        \"\"\" Extract & clean sentences from raw text of pdf. \"\"\"\n",
    "        # Remove non ASCII characters\n",
    "        printables = set(string.printable)\n",
    "        self.text = \"\".join(filter(lambda x: x in printables, self.text))\n",
    "\n",
    "        # Replace tabs with spaces\n",
    "        self.text = re.sub(r\"\\t+\", r\" \", self.text)\n",
    "\n",
    "        # Aggregate lines where the sentence wraps\n",
    "        # Also, lines in CAPITALS is counted as a header\n",
    "        fragments = []\n",
    "        prev = \"\"\n",
    "        for line in re.split(r\"\\n+\", self.text):\n",
    "            if line.isupper():\n",
    "                prev = \".\"  # skip it\n",
    "            elif line and (line.startswith(\" \") or line[0].islower()\n",
    "                  or not prev.endswith(\".\")):\n",
    "                prev = f\"{prev} {line}\"  # make into one line\n",
    "            else:\n",
    "                fragments.append(prev)\n",
    "                prev = line\n",
    "        fragments.append(prev)\n",
    "\n",
    "        # Clean the lines into sentences\n",
    "        sentences = []\n",
    "        for line in fragments:\n",
    "            # Use regular expressions to clean text\n",
    "            url_str = (r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.\"\n",
    "                       r\"([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\")\n",
    "            line = re.sub(url_str, r\" \", line)  # URLs\n",
    "            line = re.sub(r\"^\\s?\\d+(.*)$\", r\"\\1\", line)  # headers\n",
    "            line = re.sub(r\"\\d{5,}\", r\" \", line)  # figures\n",
    "            line = re.sub(r\"\\.+\", \".\", line)  # multiple periods\n",
    "            \n",
    "            line = line.strip()  # leading & trailing spaces\n",
    "            line = re.sub(r\"\\s+\", \" \", line)  # multiple spaces\n",
    "            line = re.sub(r\"\\s?([,:;\\.])\", r\"\\1\", line)  # punctuation spaces\n",
    "            line = re.sub(r\"\\s?-\\s?\", \"-\", line)  # split-line words\n",
    "\n",
    "            # Use nltk to split the line into sentences\n",
    "            for sentence in nltk.sent_tokenize(line):\n",
    "                s = str(sentence).strip().lower()  # lower case\n",
    "                # Exclude tables of contents and short sentences\n",
    "                if \"table of contents\" not in s and len(s) > 5:\n",
    "                    sentences.append(s)\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: McDonald's\n",
    "Here, we're pulling McDonalds' most recent CSR report from [responsibilityreports.com](https://www.responsibilityreports.com/Company/mcdonalds-corporation). We will extract and parse the text in order to move on to classifying it using zero shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 09:06:46,246 [MainThread  ] [INFO ]  Retrieving https://www.responsibilityreports.com/Click/2534 to C:\\Users\\USER\\AppData\\Local\\Temp/click-2534.\n",
      "2023-08-09 09:06:52,062 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The McDonalds CSR report has 288 sentences\n"
     ]
    }
   ],
   "source": [
    "mcdonalds_url = \"https://www.responsibilityreports.com/Click/2534\"\n",
    "pp = parsePDF(mcdonalds_url)\n",
    "pp.extract_contents()\n",
    "sentences = pp.clean_text()\n",
    "\n",
    "print(f\"The McDonalds CSR report has {len(sentences):,d} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Learning\n",
    "Zero-shot learning models are extremely helpful when you want to classify text on very specific labels and don't have labeled data. Labeled data can be difficult, expensive, and tedious to acquire, so zero-shot learning provides a quick way to get a classification without specialized data and additional model training.   \n",
    "\n",
    "We are going to define industry-specific ESG categories and ask our model to classify each sentence in our CSR report. We will get a \"score\" that shows how confident the model is that that label applies. A score of 1.0 means that that sentence is definitely about that topic. Conversely, a score of 0.0 means that the sentence definitely doesn't relate to that topic.  \n",
    "\n",
    "The downside to zero-shot learning is that it is extremely slow compared to models trained on specific labels. It basically has to compute \"what it means to be that label\" then it has to check if your sentence \"is that label.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotClassifier:\n",
    "\n",
    "    def create_zsl_model(self, model_name):\n",
    "        \"\"\" Create the zero-shot learning model. \"\"\"\n",
    "        self.model = pipeline(\"zero-shot-classification\", model=model_name)\n",
    "    \n",
    "        \n",
    "    def classify_text(self, text, categories):\n",
    "        \"\"\"\n",
    "        Classify text(s) to the pre-defined categories using a\n",
    "        zero-shot classification model and return the raw results.\n",
    "        \"\"\"\n",
    "        # Classify text using the zero-shot transformers model\n",
    "        hypothesis_template = \"This text is about {}.\"\n",
    "        result = self.model(text, categories, multi_label=True,\n",
    "                            hypothesis_template=hypothesis_template)\n",
    "        return result\n",
    "\n",
    "    \n",
    "    def text_labels(self, text, category_dict, cutoff=None):\n",
    "        \"\"\"\n",
    "        Classify a text into the pre-defined categories. If cutoff\n",
    "        is defined, return only those entries where the score > cutoff\n",
    "        \"\"\"\n",
    "        # Run the model on our categories\n",
    "        categories = list(category_dict.keys())\n",
    "        result = (self.classify_text(text, categories))\n",
    "        \n",
    "        # Format as a pandas dataframe and add ESG label\n",
    "        df = pd.DataFrame(result).explode([\"labels\", \"scores\"])\n",
    "        df[\"ESG\"] = df.labels.map(category_dict)\n",
    "    \n",
    "        # If a cutoff is provided, filter the dataframe\n",
    "        if cutoff:\n",
    "            df = df[df.scores.gt(cutoff)].copy()\n",
    "        return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-Define Labels\n",
    "The labels chosen below are based on categories and topics used by ESG scoring companies.  \n",
    "We define the plain-english version, which is what will be searched by the zero-shot learning model, as well as the general \"ESG\" label.  \n",
    "\n",
    "Because of how zero-shot learning models work, inference time will increase linearly with the number of labels you define. Therefore, it is necessary to consider which labels you really want and how much time is acceptable for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categories we want to classify\n",
    "esg_categories = {\n",
    "  \"emissions\": \"E\",\n",
    "  \"natural resources\": \"E\",\n",
    "  \"pollution\": \"E\",\n",
    "  \"diversity and inclusion\": \"S\",\n",
    "  \"philanthropy\": \"S\",\n",
    "  \"health and safety\": \"S\",\n",
    "  \"training and education\": \"S\",\n",
    "  \"transparancy\": \"G\",\n",
    "  \"corporate compliance\": \"G\",\n",
    "  \"board accountability\": \"G\",\n",
    "\"community engagement\": \"S\",  \n",
    "  \"data privacy\": \"G\",  \n",
    "  \"product safety\": \"E\",\n",
    "  \"renewable energy\": \"E\",  \n",
    "  \"corporate ethics\": \"G\",  \n",
    "  \"waste management\": \"E\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting Text Classification\n",
    "Now, all we have to do is define the model and make predictions. The architecture of the model can be chosen from any text-classification model on [Hugging Face](https://huggingface.co/models).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "# Define and Create the zero-shot learning model\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\" \n",
    "ZSC = ZeroShotClassifier()\n",
    "ZSC.create_zsl_model(model_name)\n",
    "    # Note: the warning is expected, so ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify all the sentences in the report\n",
    "    # Note: this takes a while\n",
    "classified = ZSC.text_labels(sentences, esg_categories)\n",
    "classified.sample(n=20)  # display 20 random records\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time in seconds\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at an example of \"E\" classified sentences:\n",
    "E_sentences = classified[classified.scores.gt(0.8) & classified.ESG.eq(\"E\")].copy()\n",
    "E_sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
